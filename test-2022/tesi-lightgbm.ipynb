{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b2d975-cdb5-4b8f-890c-a3f7af194d9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from shap import plots, Explanation\n",
    "import fasttreeshap\n",
    "import time\n",
    "import shap\n",
    "from tqdm import tqdm\n",
    "from matplotlib import ticker\n",
    "import pickle\n",
    "import lightgbm as lgb\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f96f3f6-0836-422d-ab56-89c40dbf8660",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to create dataframe with metrics\n",
    "def performanceMetricsDF(metricsObj, yTrain, yPredTrain, yTest, yPredTest, average='binary'):\n",
    "  measures_list = ['ACCURACY','PRECISION', 'RECALL','F1 SCORE','AUC']\n",
    "  train_results = [metricsObj.accuracy_score(yTrain, yPredTrain),\n",
    "                metricsObj.precision_score(yTrain, yPredTrain, average = average),\n",
    "                metricsObj.recall_score(yTrain, yPredTrain, average = average),\n",
    "                metricsObj.f1_score(yTrain, yPredTrain, average = average),\n",
    "                metricsObj.roc_auc_score(yTrain, yPredTrain, average = None if average == 'binary' else average)\n",
    "                ]\n",
    "  test_results = [metricsObj.accuracy_score(yTest, yPredTest),\n",
    "               metricsObj.precision_score(yTest, yPredTest, average = average),\n",
    "               metricsObj.recall_score(yTest, yPredTest, average = average),\n",
    "               metricsObj.f1_score(yTest, yPredTest, average = average),\n",
    "               metricsObj.roc_auc_score(yTest, yPredTest, average = None if average == 'binary' else average)\n",
    "               ]\n",
    "  resultsDF = pd.DataFrame({'Measure': measures_list, 'Train': train_results, 'Test':test_results})\n",
    "  return(resultsDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1cdfa7-448c-4748-816e-9fd5ca9d31c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to plot confusion matrix - Adapted from https://github.com/DTrimarchi10/confusion_matrix/blob/master/cf_matrix.py\n",
    "def make_confusion_matrix(cf,\n",
    "                          group_names=None,\n",
    "                          categories='auto',\n",
    "                          count=True,\n",
    "                          percent=True,\n",
    "                          cbar=True,\n",
    "                          xyticks=True,\n",
    "                          xyplotlabels=True,\n",
    "                          sum_stats=True,\n",
    "                          figsize=None,\n",
    "                          cmap='Blues',\n",
    "                          title=None):\n",
    "    '''\n",
    "    This function will make a pretty plot of an sklearn Confusion Matrix cm using a Seaborn heatmap visualization.\n",
    "    Arguments\n",
    "    ---------\n",
    "    cf:            confusion matrix to be passed in\n",
    "    group_names:   List of strings that represent the labels row by row to be shown in each square.\n",
    "    categories:    List of strings containing the categories to be displayed on the x,y axis. Default is 'auto'\n",
    "    count:         If True, show the raw number in the confusion matrix. Default is True.\n",
    "    normalize:     If True, show the proportions for each category. Default is True.\n",
    "    cbar:          If True, show the color bar. The cbar values are based off the values in the confusion matrix.\n",
    "                   Default is True.\n",
    "    xyticks:       If True, show x and y ticks. Default is True.\n",
    "    xyplotlabels:  If True, show 'True Label' and 'Predicted Label' on the figure. Default is True.\n",
    "    sum_stats:     If True, display summary statistics below the figure. Default is True.\n",
    "    figsize:       Tuple representing the figure size. Default will be the matplotlib rcParams value.\n",
    "    cmap:          Colormap of the values displayed from matplotlib.pyplot.cm. Default is 'Blues'\n",
    "                   See http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "                   \n",
    "    title:         Title for the heatmap. Default is None.\n",
    "    '''\n",
    "\n",
    "\n",
    "    # CODE TO GENERATE TEXT INSIDE EACH SQUARE\n",
    "    blanks = ['' for i in range(cf.size)]\n",
    "\n",
    "    if group_names and len(group_names)==cf.size:\n",
    "        group_labels = [\"{}\\n\".format(value) for value in group_names]\n",
    "    else:\n",
    "        group_labels = blanks\n",
    "\n",
    "    if count:\n",
    "        group_counts = [\"{0:0.0f}\\n\".format(value) for value in cf.flatten()]\n",
    "    else:\n",
    "        group_counts = blanks\n",
    "\n",
    "    if percent:\n",
    "        group_percentages = [\"{0:.2%}\".format(value) for value in cf.flatten()/np.sum(cf)]\n",
    "    else:\n",
    "        group_percentages = blanks\n",
    "\n",
    "    box_labels = [f\"{v1}{v2}{v3}\".strip() for v1, v2, v3 in zip(group_labels,group_counts,group_percentages)]\n",
    "    box_labels = np.asarray(box_labels).reshape(cf.shape[0],cf.shape[1])\n",
    "\n",
    "\n",
    "    # CODE TO GENERATE SUMMARY STATISTICS & TEXT FOR SUMMARY STATS\n",
    "    if sum_stats:\n",
    "        #Accuracy is sum of diagonal divided by total observations\n",
    "        accuracy  = np.trace(cf) / float(np.sum(cf))\n",
    "\n",
    "        #if it is a binary confusion matrix, show some more stats\n",
    "        if len(cf)==2:\n",
    "            #Metrics for Binary Confusion Matrices\n",
    "            precision = cf[1,1] / sum(cf[:,1])\n",
    "            recall    = cf[1,1] / sum(cf[1,:])\n",
    "            f1_score  = 2*precision*recall / (precision + recall)\n",
    "            stats_text = \"\\n\\nAccuracy={:0.3f}\\nPrecision={:0.3f}\\nRecall={:0.3f}\\nF1 Score={:0.3f}\".format(\n",
    "                accuracy,precision,recall,f1_score)\n",
    "        else:\n",
    "            stats_text = \"\\n\\nAccuracy={:0.3f}\".format(accuracy)\n",
    "    else:\n",
    "        stats_text = \"\"\n",
    "\n",
    "\n",
    "    # SET FIGURE PARAMETERS ACCORDING TO OTHER ARGUMENTS\n",
    "    if figsize==None:\n",
    "        #Get default figure size if not set\n",
    "        figsize = plt.rcParams.get('figure.figsize')\n",
    "\n",
    "    if xyticks==False:\n",
    "        #Do not show categories if xyticks is False\n",
    "        categories=False\n",
    "\n",
    "\n",
    "    # MAKE THE HEATMAP VISUALIZATION\n",
    "    plt.figure(figsize=figsize)\n",
    "    ax = sns.heatmap(cf,annot=box_labels, fmt=\"\",cmap=cmap,cbar=cbar,xticklabels=categories,yticklabels=categories)\n",
    "\n",
    "    if xyplotlabels:\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label' + stats_text)\n",
    "    else:\n",
    "        plt.xlabel(stats_text)\n",
    "    \n",
    "    if title:\n",
    "        plt.title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336c62c3-860a-4498-8481-81f68e4e06a0",
   "metadata": {},
   "source": [
    "- aggiungere cross validation\n",
    "- passare a usare i sample\n",
    "- guardare la dimesione dell'albero\n",
    "- confrontare sample con dati interi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a371d5e1-b249-4dd3-8840-3c77820acd38",
   "metadata": {},
   "source": [
    "Here I change the type of some feature becuase since they come from a network package they are supposed to be a certain amount of bit maximum, I also checked before to do the change.\n",
    "\n",
    "Then I will eclude the ip of the hosts, the port and the Unnamed: 0. Because the ip and ports are categorical but they are to many to fit in the model, and also there is not a good reason for train the model over the ip since it change based on the network so the attacker will always have a different one. About the Unnamed: 0 you can use that number to split this csv in mani csvs which is not a thing that we need to do so I removed that feature as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4a836a-9bf1-4ad7-a2fd-2357252a5860",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dtype_dict = {\n",
    "    'Unnamed: 0': 'uint32',\n",
    "    'uid': 'str',\n",
    "    'originh': 'category',\n",
    "    'originp': 'uint16',\n",
    "    'responh': 'category',\n",
    "    'responp': 'uint16',\n",
    "    'flow_duration': 'float64',\n",
    "    'fwd_pkts_tot': 'uint64',\n",
    "    'bwd_pkts_tot': 'uint64',\n",
    "    'fwd_data_pkts_tot': 'uint64',\n",
    "    'bwd_data_pkts_tot': 'uint64',\n",
    "    'fwd_pkts_per_sec': 'float64',\n",
    "    'bwd_pkts_per_sec': 'float64',\n",
    "    'flow_pkts_per_sec': 'float64',\n",
    "    'down_up_ratio': 'float32',\n",
    "    'fwd_header_size_tot': 'uint64',\n",
    "    'fwd_header_size_min': 'uint8',\n",
    "    'fwd_header_size_max': 'uint8',\n",
    "    'bwd_header_size_tot': 'uint64',\n",
    "    'bwd_header_size_min': 'uint8',\n",
    "    'bwd_header_size_max': 'uint8',\n",
    "    'flow_FIN_flag_count': 'uint64',\n",
    "    'flow_SYN_flag_count': 'uint64',\n",
    "    'flow_RST_flag_count': 'uint64',\n",
    "    'fwd_PSH_flag_count': 'uint64',\n",
    "    'bwd_PSH_flag_count': 'uint64',\n",
    "    'flow_ACK_flag_count': 'uint64',\n",
    "    'fwd_URG_flag_count': 'uint64',\n",
    "    'bwd_URG_flag_count': 'uint64',\n",
    "    'flow_CWR_flag_count': 'uint64',\n",
    "    'flow_ECE_flag_count': 'uint64',\n",
    "    'fwd_pkts_payload.min': 'uint16',\n",
    "    'fwd_pkts_payload.max': 'uint16',\n",
    "    'fwd_pkts_payload.tot': 'float64',\n",
    "    'fwd_pkts_payload.avg': 'float64',\n",
    "    'fwd_pkts_payload.std': 'float64',\n",
    "    'bwd_pkts_payload.min': 'uint16',\n",
    "    'bwd_pkts_payload.max': 'uint16',\n",
    "    'bwd_pkts_payload.tot': 'float64',\n",
    "    'bwd_pkts_payload.avg': 'float64',\n",
    "    'bwd_pkts_payload.std': 'float64',\n",
    "    'flow_pkts_payload.min': 'uint16',\n",
    "    'flow_pkts_payload.max': 'uint16',\n",
    "    'flow_pkts_payload.tot': 'float64',\n",
    "    'flow_pkts_payload.avg': 'float64',\n",
    "    'flow_pkts_payload.std': 'float64',\n",
    "    'fwd_iat.min': 'float64',\n",
    "    'fwd_iat.max': 'float64',\n",
    "    'fwd_iat.tot': 'float64',\n",
    "    'fwd_iat.avg': 'float64',\n",
    "    'fwd_iat.std': 'float64',\n",
    "    'bwd_iat.min': 'float64',\n",
    "    'bwd_iat.max': 'float64',\n",
    "    'bwd_iat.tot': 'float64',\n",
    "    'bwd_iat.avg': 'float64',\n",
    "    'bwd_iat.std': 'float64',\n",
    "    'flow_iat.min': 'float64',\n",
    "    'flow_iat.max': 'float64',\n",
    "    'flow_iat.tot': 'float64',\n",
    "    'flow_iat.avg': 'float64',\n",
    "    'flow_iat.std': 'float64',\n",
    "    'payload_bytes_per_second': 'float64',\n",
    "    'fwd_subflow_pkts': 'float64',\n",
    "    'bwd_subflow_pkts': 'float64',\n",
    "    'fwd_subflow_bytes': 'float64',\n",
    "    'bwd_subflow_bytes': 'float64',\n",
    "    'fwd_bulk_bytes': 'float64',\n",
    "    'bwd_bulk_bytes': 'float64',\n",
    "    'fwd_bulk_packets': 'float32',\n",
    "    'bwd_bulk_packets': 'float32',\n",
    "    'fwd_bulk_rate': 'float64',\n",
    "    'bwd_bulk_rate': 'float64',\n",
    "    'active.min': 'float64',\n",
    "    'active.max': 'float64',\n",
    "    'active.tot': 'float64',\n",
    "    'active.avg': 'float64',\n",
    "    'active.std': 'float64',\n",
    "    'idle.min': 'float64',\n",
    "    'idle.max': 'float64',\n",
    "    'idle.tot': 'float64',\n",
    "    'idle.avg': 'float64',\n",
    "    'idle.std': 'float64',\n",
    "    'fwd_init_window_size': 'uint16',\n",
    "    'bwd_init_window_size': 'uint16',\n",
    "    'fwd_last_window_size': 'uint16',\n",
    "    'traffic_category': 'category',\n",
    "    'Label': 'bool'\n",
    "}\n",
    "\n",
    "selected_features = [\n",
    "    \"flow_duration\", \"fwd_pkts_tot\", \"bwd_pkts_tot\",\n",
    "    \"fwd_data_pkts_tot\", \"bwd_data_pkts_tot\", \"fwd_pkts_per_sec\", \"bwd_pkts_per_sec\", \"flow_pkts_per_sec\",\n",
    "    \"down_up_ratio\", \"fwd_header_size_tot\", \"fwd_header_size_min\", \"fwd_header_size_max\",\n",
    "    \"bwd_header_size_tot\", \"bwd_header_size_min\", \"bwd_header_size_max\", \"flow_FIN_flag_count\",\n",
    "    \"flow_SYN_flag_count\", \"flow_RST_flag_count\", \"fwd_PSH_flag_count\", \"bwd_PSH_flag_count\", \"flow_ACK_flag_count\",\n",
    "    \"fwd_URG_flag_count\", \"bwd_URG_flag_count\", \"flow_CWR_flag_count\", \"flow_ECE_flag_count\",\n",
    "    \"fwd_pkts_payload.min\", \"fwd_pkts_payload.max\", \"fwd_pkts_payload.tot\", \"fwd_pkts_payload.avg\",\n",
    "    \"fwd_pkts_payload.std\", \"bwd_pkts_payload.min\", \"bwd_pkts_payload.max\", \"bwd_pkts_payload.tot\",\n",
    "    \"bwd_pkts_payload.avg\", \"bwd_pkts_payload.std\", \"flow_pkts_payload.min\", \"flow_pkts_payload.max\",\n",
    "    \"flow_pkts_payload.tot\", \"flow_pkts_payload.avg\", \"flow_pkts_payload.std\", \"fwd_iat.min\",\n",
    "    \"fwd_iat.max\", \"fwd_iat.tot\", \"fwd_iat.avg\", \"fwd_iat.std\", \"bwd_iat.min\", \"bwd_iat.max\",\n",
    "    \"bwd_iat.tot\", \"bwd_iat.avg\", \"bwd_iat.std\", \"flow_iat.min\", \"flow_iat.max\", \"flow_iat.tot\",\n",
    "    \"flow_iat.avg\", \"flow_iat.std\", \"payload_bytes_per_second\", \"fwd_subflow_pkts\", \"bwd_subflow_pkts\",\n",
    "    \"fwd_subflow_bytes\", \"bwd_subflow_bytes\", \"fwd_bulk_bytes\", \"bwd_bulk_bytes\", \"fwd_bulk_packets\",\n",
    "    \"bwd_bulk_packets\", \"fwd_bulk_rate\", \"bwd_bulk_rate\", \"active.min\", \"active.max\", \"active.tot\",\n",
    "    \"active.avg\", \"active.std\", \"idle.min\", \"idle.max\", \"idle.tot\", \"idle.avg\", \"idle.std\",\n",
    "    \"fwd_init_window_size\", \"bwd_init_window_size\", \"fwd_last_window_size\", \"traffic_category\", \"Label\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765ecb4e-17ba-4508-bd1b-f8cdad0022ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# you should run making_hikari2022_csv.ipynb before running this cell, since the two csv given with the paper have two differences\n",
    "ds = pd.concat([\n",
    "    pd.read_csv('../datasets/HIKARI-2021/ALLFLOWMETER_HIKARI2021.csv', dtype=dtype_dict, usecols=selected_features),\n",
    "    pd.read_csv('../datasets/HIKARI-2021/ALLFLOWMETER_HIKARI2022.csv', dtype=dtype_dict, usecols=selected_features)], \n",
    "    ignore_index=True)\n",
    "ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2956c743-de3f-4ac5-8c96-7be137993f0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b8c249-5c51-4b75-8789-3673890137e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8882120c-3a52-4064-8dfc-3e43267805f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2684c53a-4a5e-4b9a-a5ac-483cc536b7a2",
   "metadata": {},
   "source": [
    "We can see from this graph that we don't have highly correlated features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da76b5ac-9078-4ef8-ab82-a13c3a7f4674",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def show_corr_matrix():\n",
    "    sns.set_theme(style=\"white\")\n",
    "\n",
    "    # Compute the correlation matrix\n",
    "    corr = ds.loc[:, ds.columns != 'traffic_category'].corr()\n",
    "\n",
    "    # Generate a mask for the upper triangle\n",
    "    mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "    # Set up the matplotlib figure\n",
    "    f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "    # Generate a custom diverging colormap\n",
    "    cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "\n",
    "    # Draw the heatmap with the mask and correct aspect ratio\n",
    "    sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "                square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "    \n",
    "show_corr_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e93d40f-ffdd-487e-a26f-04a3fbc548c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_ratio(data):\n",
    "    # Get ratio instead of raw numbers using normalize=True\n",
    "    ratio = data['traffic_category'].value_counts(normalize=True)\n",
    "\n",
    "    # Round and then convert to percentage\n",
    "    ratio = ratio.round(4)*100\n",
    "\n",
    "    # convert to a DataFrame and store in variable 'traffic_category_ratios'\n",
    "    # We'll use this variable to compare ratios for samples \n",
    "    # selected using SRS and Stratified Sampling \n",
    "    traffic_category_ratios = pd.DataFrame({'Ratio':ratio})\n",
    "    print(traffic_category_ratios)\n",
    "    \n",
    "compute_ratio(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96be676-bfc7-46f9-a2e2-fcbb0a7ff6ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_features = [    \n",
    "    \"flow_duration\", \"fwd_pkts_tot\", \"bwd_pkts_tot\",\n",
    "    \"fwd_data_pkts_tot\", \"bwd_data_pkts_tot\", \"fwd_pkts_per_sec\", \"bwd_pkts_per_sec\", \"flow_pkts_per_sec\",\n",
    "    \"down_up_ratio\", \"fwd_header_size_tot\", \"fwd_header_size_min\", \"fwd_header_size_max\",\n",
    "    \"bwd_header_size_tot\", \"bwd_header_size_min\", \"bwd_header_size_max\", \"flow_FIN_flag_count\",\n",
    "    \"flow_SYN_flag_count\", \"flow_RST_flag_count\", \"fwd_PSH_flag_count\", \"bwd_PSH_flag_count\", \"flow_ACK_flag_count\",\n",
    "    \"fwd_URG_flag_count\", \"bwd_URG_flag_count\", \"flow_CWR_flag_count\", \"flow_ECE_flag_count\",\n",
    "    \"fwd_pkts_payload.min\", \"fwd_pkts_payload.max\", \"fwd_pkts_payload.tot\", \"fwd_pkts_payload.avg\",\n",
    "    \"fwd_pkts_payload.std\", \"bwd_pkts_payload.min\", \"bwd_pkts_payload.max\", \"bwd_pkts_payload.tot\",\n",
    "    \"bwd_pkts_payload.avg\", \"bwd_pkts_payload.std\", \"flow_pkts_payload.min\", \"flow_pkts_payload.max\",\n",
    "    \"flow_pkts_payload.tot\", \"flow_pkts_payload.avg\", \"flow_pkts_payload.std\", \"fwd_iat.min\",\n",
    "    \"fwd_iat.max\", \"fwd_iat.tot\", \"fwd_iat.avg\", \"fwd_iat.std\", \"bwd_iat.min\", \"bwd_iat.max\",\n",
    "    \"bwd_iat.tot\", \"bwd_iat.avg\", \"bwd_iat.std\", \"flow_iat.min\", \"flow_iat.max\", \"flow_iat.tot\",\n",
    "    \"flow_iat.avg\", \"flow_iat.std\", \"payload_bytes_per_second\", \"fwd_subflow_pkts\", \"bwd_subflow_pkts\",\n",
    "    \"fwd_subflow_bytes\", \"bwd_subflow_bytes\", \"fwd_bulk_bytes\", \"bwd_bulk_bytes\", \"fwd_bulk_packets\",\n",
    "    \"bwd_bulk_packets\", \"fwd_bulk_rate\", \"bwd_bulk_rate\", \"active.min\", \"active.max\", \"active.tot\",\n",
    "    \"active.avg\", \"active.std\", \"idle.min\", \"idle.max\", \"idle.tot\", \"idle.avg\", \"idle.std\",\n",
    "    \"fwd_init_window_size\", \"bwd_init_window_size\", \"fwd_last_window_size\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc979f8-da87-4b78-aefe-128a26ea323f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "category_size = 7988\n",
    "sampling_weights = {'Background': category_size*2, 'Benign': category_size*2, 'XMRIGCC CryptoMiner': category_size, 'Probing': category_size, 'Bruteforce': category_size, 'Bruteforce-XML': category_size}\n",
    "\n",
    "rus = RandomUnderSampler(random_state=1, sampling_strategy=sampling_weights)\n",
    "X_res, y_res = rus.fit_resample(ds[x_features], ds.traffic_category)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8726694-1ba2-4271-a953-3f7904cbd43b",
   "metadata": {},
   "source": [
    "ricordarsi di fare fit una volta sola per tutti i dati e poi usare transform dove serve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b4b2ac-c372-4355-bfc7-e72643647c33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_res, ds.loc[y_res.index].Label, test_size = 0.2, random_state=12,  stratify=ds.loc[y_res.index].traffic_category)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587972b1-1ae4-422e-8c59-5d857bd75f6a",
   "metadata": {},
   "source": [
    "This random forest with max depth 6 is better then any other max depth, and by descreasing the number of estimator to two we can see that the f1 is still 0,96 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82856e69-c594-4aad-9797-e117190ff039",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def stratified_under_sample(group: pd.DataFrame, k: int, random_state: int):\n",
    "    global category_size\n",
    "    # shuffle data\n",
    "    group = group.sample(frac=1, random_state=random_state)\n",
    "    \n",
    "    # making a dictionary for checking if all the groups are equally insert into the array\n",
    "    unique_categories = set(group)\n",
    "    \n",
    "    # getting the size of each category per fold\n",
    "    folded_category = category_size // k\n",
    "    \n",
    "    # storing temporary data\n",
    "    test_res = []\n",
    "    train_res = []\n",
    "\n",
    "    for i in range(k):\n",
    "        test_indexes = []\n",
    "        train_indexes = []\n",
    "        \n",
    "        # for each iteration of the outer loop we need to reset dict\n",
    "        count_type = {category: 0 for category in unique_categories}\n",
    "        \n",
    "        for category in unique_categories:\n",
    "            # making a window of data to retreive\n",
    "            if (category == 'Background') | (category == 'Benign'):\n",
    "                start = (folded_category * 2) * i\n",
    "                stop = (folded_category * 2) * (i + 1)\n",
    "            else:\n",
    "                start = folded_category * i\n",
    "                stop = folded_category * (i + 1)\n",
    "            test_indexes.extend(group[group == category].iloc[start:stop].index)\n",
    "        \n",
    "        for x in group.index:\n",
    "            if x not in test_indexes:\n",
    "                train_indexes.append(x)\n",
    "                \n",
    "        # shuffling the data with the same seed in order to have the same result in both the dataset\n",
    "        np.random.shuffle(test_indexes)\n",
    "        test_res.append(test_indexes)\n",
    "        np.random.shuffle(train_indexes)\n",
    "        train_res.append(train_indexes)\n",
    "        \n",
    "    return test_res, train_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5fec86-2057-4684-982d-9a820543939a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "param = {\n",
    "    'boosting_type': 'gbdt',  # Gradient Boosting Decision Tree\n",
    "    'objective': 'binary',  # Binary classification\n",
    "    'metric': 'f1',  # F1 score as the evaluation metric\n",
    "    'num_leaves': 16,  # Maximum number of leaves in one tree\n",
    "    'verbose': 0,  # Verbosity\n",
    "    'num_iterations': 24,  # Number of trees to train\n",
    "    'max_depth': 6,  # Maximum depth of trees\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b42578c-e368-4fd1-ae4f-40129efd84dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cross_validation(X, y, group, verbose):\n",
    "    test_kfold, train_kfold = stratified_under_sample(group, 10, 12)\n",
    "    \n",
    "    cvscores = []\n",
    "\n",
    "    start_cv = time.time()\n",
    "    for test, train in zip(test_kfold, train_kfold):\n",
    "        train_data = lgb.Dataset(X.loc[train], label=y.loc[train])\n",
    "        \n",
    "        bst = lgb.train(param, train_data)\n",
    "        \n",
    "        y_predicted = bst.predict(X.loc[test]) >= 0.5\n",
    "\n",
    "        cvscores.append(metrics.f1_score(y.loc[test], y_predicted))\n",
    "    end_cv = time.time()    \n",
    "    \n",
    "    return np.mean(cvscores), np.std(cvscores), end_cv - start_cv\n",
    "        \n",
    "%time cross_validation(ds[x_features], ds.Label, y_res, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd4da16-87bf-4801-b792-c3367cfb3491",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = lgb.Dataset(X_train, label=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f44ebe-de0f-4587-80b1-54ac284bb110",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "bst = lgb.train(param, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58aaf40e-83d0-40f5-a455-4003c4e25291",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%time y_predicted_train = bst.predict(X_train) >= 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edea7a82-7a17-44ba-bd6d-b26aeada1a2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%time y_predicted_test = bst.predict(X_test) >= 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6570f472-ae25-415d-8f5c-338f08f29423",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "performanceMetricsDF(metrics, y_train, y_predicted_train, y_test, y_predicted_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881140ba-412e-42c4-a09b-b3baf19713c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Show the confusion matrix\n",
    "cf = metrics.confusion_matrix(y_test,y_predicted_test)\n",
    "labels = ['True Neg','False Pos','False Neg','True Pos']\n",
    "categories = ['0', '1']\n",
    "make_confusion_matrix(cf, \n",
    "                      group_names=labels,\n",
    "                      categories=categories,\n",
    "                      cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ec4235-b6cb-43ea-89ef-dc7dca944e11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a plot for each tree in the model\n",
    "for i in range(bst.num_trees()):\n",
    "    graph = lgb.create_tree_digraph(bst, tree_index=i, name='Tree {}'.format(i), show_info=['split_gain', 'internal_value', 'leaf_count'])\n",
    "    graph.render(filename='tree_{}'.format(i), directory='lgb/tree', format='pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9f0306-82a0-4f96-85dd-0071ecacee82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lgb.plot_importance(bst, figsize=(20, 16))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ed89e0-4e65-4100-8c26-816b11456f3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "explainer = fasttreeshap.TreeExplainer(bst, X_train, algorithm='auto', n_jobs=-1, feature_perturbation=\"interventional\")\n",
    "sv = explainer.shap_values(X_train)\n",
    "\n",
    "exp = Explanation(sv, \n",
    "                  np.mean(bst.predict(X_train) >= 0.5), \n",
    "                  data=X_train.values, \n",
    "                  feature_names=x_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c3b8ae-16d3-4a95-a18a-c611095fe7a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plots.bar(exp, max_display=20, show=False)\n",
    "# plt.savefig('tree/shap', dpi=1400, format='pdf', pad_inches=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f95fa7f-1845-4875-b77b-ec50e0ad726e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plots.beeswarm(exp,order=exp.abs.max(0),max_display=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fecc16-0b53-481e-9142-8d66a32a55be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_importance = pd.DataFrame(data={'importance' : exp.abs.mean(0).values, 'feature': x_features})\n",
    "feature_importance.sort_values('importance',ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646da0cb-a3e6-41f5-9c6a-9a59350b7211",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_above_zero = feature_importance.query('importance > 0').sort_values('importance',ascending=False)['feature'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f13dd72-6ffb-47c6-a7a4-913859483bab",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def get_score(features):\n",
    "    X = X_res[features]\n",
    "    y = ds.loc[y_res.index].Label\n",
    "    cv_mean, cv_std, cv_time = cross_validation(X, y, y_res, 0)\n",
    "    return cv_mean, cv_std, len(features), cv_time\n",
    "\n",
    "def recursive_reduction():\n",
    "    scores = []\n",
    "    score_std = []\n",
    "    n_features = []\n",
    "    cv_time = []\n",
    "\n",
    "    # making a warm up run otherwise the first one will be always slower than the others\n",
    "    # only one features so that it can be as fast as possibile\n",
    "    get_score(['fwd_iat.tot'])\n",
    "\n",
    "    result = get_score(feature_above_zero)\n",
    "    scores.append(result[0])\n",
    "    score_std.append(result[1])\n",
    "    n_features.append(result[2])    \n",
    "    cv_time.append(result[3])\n",
    "    \n",
    "    for i in range(1,len(feature_above_zero)):\n",
    "        print(f\"testing with {len(feature_above_zero[:-i])} features\")\n",
    "        result = get_score(feature_above_zero[:-i])\n",
    "        scores.append(result[0])\n",
    "        score_std.append(result[1])\n",
    "        n_features.append(result[2])\n",
    "        cv_time.append(result[3])\n",
    "        \n",
    "    return scores, score_std, n_features, cv_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547ff383-a493-4285-9d18-eb00e50f3da0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    result = recursive_reduction()\n",
    "    feature_reduction_scores = pd.DataFrame({'F1_score': result[0], 'score_std': result[1], 'n_features': result[2], 'cv_time': result[3]})\n",
    "    with open('lgb_data/feature_reduction_scores.pickle', 'wb') as handle:\n",
    "        pickle.dump(feature_reduction_scores, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "else:\n",
    "    with open('lgb_data/feature_reduction_scores.pickle', 'rb') as handle:\n",
    "        feature_reduction_scores = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbedbc2-c993-4e6e-8af4-39f58b57e874",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.set_theme()\n",
    "sns.lineplot(data=feature_reduction_scores, x=\"n_features\", y=\"cv_time\")\n",
    "sns.regplot(data=feature_reduction_scores, x=\"n_features\", y=\"cv_time\", ci=None)  # ci=None removes confidence intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bbb909-23f0-4024-b754-ebe98711893b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.set_theme()\n",
    "sns.lineplot(data=feature_reduction_scores, x=\"n_features\", y=\"F1_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc38614-fe94-497a-ba2d-e7a149400910",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.set_theme()\n",
    "sns.lineplot(data=feature_reduction_scores, x=\"n_features\", y=\"score_std\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc8fa51-5d16-49e3-b0e0-09ddf7eece1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_reduction_scores.sort_values('F1_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7ec7bc-936f-4763-8978-d76fa25739ca",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# add also training time\n",
    "attack_f1 = []\n",
    "attack_recall = []\n",
    "attack_precision = []\n",
    "cv_score_avg = []\n",
    "cv_score_std = []\n",
    "n_features = []\n",
    "attacks = []\n",
    "fit_time = []\n",
    "pred_time = []\n",
    "\n",
    "# warmup boolean, this variable will be used to load in memory the function in order to have reliable time measures\n",
    "warmup = True\n",
    "\n",
    "\n",
    "def test_zero_day(attack, features, rus, rus_attack):\n",
    "    print(f\"training with {len(features)} features\")\n",
    "    global warmup\n",
    "    \n",
    "    X_res, y_res = rus.fit_resample(ds[features], ds.traffic_category)\n",
    "    \n",
    "    X_attack, y_attack = rus_attack.fit_resample(ds[features], ds.traffic_category)\n",
    "    y_attack = ds.loc[y_attack.index].Label\n",
    "    \n",
    "    cv_mean, cv_std, cv_time = cross_validation(X_res, ds.loc[y_res.index].Label, y_res, 0)\n",
    "    X_res = lgb.Dataset(X_res, label=ds.loc[y_res.index].Label)\n",
    "    \n",
    "    if warmup:\n",
    "        bst = lgb.train(param, X_res)\n",
    "        bst.predict(X_attack)\n",
    "    \n",
    "    start_fit = time.time()\n",
    "    bst = lgb.train(param, X_res)\n",
    "    end_fit = time.time()\n",
    "    \n",
    "    start_pred = time.time()\n",
    "    y_predicted = bst.predict(X_attack)\n",
    "    end_pred = time.time()\n",
    "    y_predicted = y_predicted >= 0.5\n",
    "    \n",
    "    attack_f1.append(metrics.f1_score(y_attack, y_predicted))\n",
    "    attack_recall.append(metrics.recall_score(y_attack, y_predicted))\n",
    "    attack_precision.append(metrics.precision_score(y_attack, y_predicted))\n",
    "    cv_score_avg.append(cv_mean)\n",
    "    cv_score_std.append(cv_std)\n",
    "    n_features.append(len(features))\n",
    "    attacks.append(attack)\n",
    "    fit_time.append(end_fit - start_fit)\n",
    "    pred_time.append(end_pred - start_pred)\n",
    "\n",
    "    # at the very first iteration we change it to false\n",
    "    warmup = False\n",
    "    \n",
    "def recursive_reduction(attack):\n",
    "    global scaler\n",
    "    # making a sample for having a 1:1 ration for positive and negative class\n",
    "    # keep in mind that in the training I will have only three attacks, while for the test only one attack\n",
    "    sampling_weights = {'Background': int(category_size * 1.5), 'Benign': int(category_size * 1.5), 'XMRIGCC CryptoMiner': category_size, 'Probing': category_size, 'Bruteforce': category_size, 'Bruteforce-XML': category_size}\n",
    "    sampling_attack = {'Background': int(category_size * 0.5), 'Benign': int(category_size * 0.5), 'XMRIGCC CryptoMiner': 0, 'Probing': 0, 'Bruteforce': 0, 'Bruteforce-XML': 0}\n",
    "    \n",
    "    # removing all the attack observations\n",
    "    sampling_weights[attack] = 0\n",
    "    \n",
    "    # doing the undersampling\n",
    "    rus = RandomUnderSampler(random_state=42, sampling_strategy=sampling_weights)\n",
    "    \n",
    "    # adding the attack to the test dataset with the non attack traffic\n",
    "    sampling_attack[attack] = 3279\n",
    "    \n",
    "    # making the dataset with only one attack\n",
    "    rus_attack = RandomUnderSampler(random_state=42, sampling_strategy=sampling_attack)\n",
    "    \n",
    "    test_zero_day(attack, feature_above_zero, rus, rus_attack)\n",
    "\n",
    "    for i in range(1,len(feature_above_zero)):\n",
    "        test_zero_day(attack, feature_above_zero[:-i], rus, rus_attack)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9474a1-fd74-4765-a948-7e6be3b191e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    for attack in ['XMRIGCC CryptoMiner','Probing','Bruteforce','Bruteforce-XML']:\n",
    "        print('traing for ', attack)\n",
    "        %time recursive_reduction(attack)\n",
    "\n",
    "    zero_day_feature_reduction_scores = pd.DataFrame({'attack_f1': attack_f1,  'attack_recall': attack_recall, 'attack_precision': attack_precision, 'cv_score_avg': cv_score_avg, 'cv_score_std': cv_score_std, 'n_features': n_features, 'attack_name': attacks, 'fit_time': fit_time, 'pred_time': pred_time})\n",
    "    with open('lgb_data/zero_day_feature_reduction_scores.pickle', 'wb') as handle:\n",
    "        pickle.dump(zero_day_feature_reduction_scores, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "else:\n",
    "    with open('lgb_data/zero_day_feature_reduction_scores.pickle', 'rb') as handle:\n",
    "        zero_day_feature_reduction_scores = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabc9e44-d55d-4cfe-9746-f0289177ff3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# here I create this detected variable, so that we can see how many attacks are detected because only by using the mean we didn't get the best one\n",
    "zero_day_feature_reduction_scores['detected'] = np.where(zero_day_feature_reduction_scores['attack_recall'] > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1964c9aa-421f-4ed6-ac52-b59fc34a2be1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "zero_day_feature_reduction_scores.query('attack_recall != 0').sort_values('attack_recall', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148d9df0-37e3-4327-8b72-c6a618186e14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_plot():\n",
    "    data = zero_day_feature_reduction_scores.groupby('n_features')[['pred_time','n_features']].mean().query('n_features < 79')\n",
    "    sns.set_theme()\n",
    "    sns.lineplot(data=data, x=data.index, y='pred_time')\n",
    "    sns.regplot(data=data, x=\"n_features\", y=\"pred_time\", ci=None)  # ci=None removes confidence intervals\n",
    "\n",
    "print_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515104be-87c3-493b-83e9-972ae5a3edeb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_plot():\n",
    "    data = zero_day_feature_reduction_scores.groupby('n_features')[['cv_score_std','n_features']].mean().query('n_features < 79')\n",
    "    sns.set_theme()\n",
    "    sns.lineplot(data=data, x=data.index, y='cv_score_std')\n",
    "\n",
    "print_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8deb5c-88da-4f83-a487-4039a847f9d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "zero_day_feature_reduction_scores.groupby('n_features')[['attack_f1','attack_recall','attack_precision','cv_score_avg','cv_score_std','fit_time','pred_time', 'detected']].mean().sort_values(['attack_recall', 'attack_precision'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a352c7d-3dd6-43e5-83e4-d4278198d880",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_plot():\n",
    "    data = zero_day_feature_reduction_scores.groupby('n_features')[['detected','n_features']].mean().query('n_features < 79')\n",
    "    sns.set_theme()\n",
    "    sns.lineplot(data=data, x=data.index, y='detected')\n",
    "\n",
    "print_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6024b9-bec8-4283-b46b-30ea30df0e0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_plot():\n",
    "    data = zero_day_feature_reduction_scores.groupby('n_features')[['attack_recall','n_features']].mean().query('n_features < 79')\n",
    "    sns.set_theme()\n",
    "    sns.lineplot(data=data, x=data.index, y='attack_recall')\n",
    "\n",
    "print_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b232b3eb-dcc9-4b5a-a0df-08cf672d23d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "zero_day_feature_reduction_scores.groupby('n_features')[['attack_f1','attack_recall','attack_precision','cv_score_avg','cv_score_std','fit_time','pred_time']].mean().sort_values('attack_f1', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0fbd8d-a7e5-4acf-bbaf-7a20c67c7be7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_plot():\n",
    "    data = zero_day_feature_reduction_scores.groupby('n_features')[['attack_f1','n_features']].mean().query('n_features < 79')\n",
    "    sns.set_theme()\n",
    "    sns.lineplot(data=data, x=data.index, y='attack_f1')\n",
    "\n",
    "print_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c58dd17-c715-48c0-ac66-85d06d65a64d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "zero_day_feature_reduction_scores.query('n_features == 4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02609a1-e163-4f8f-bc3a-3ac48bad6d77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "zero_day_feature_reduction_scores.query('n_features == 6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb48f118-5b3e-41f6-8689-c8ac1df72330",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
